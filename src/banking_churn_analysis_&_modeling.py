# -*- coding: utf-8 -*-
"""Banking Churn Analysis & Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bDMAH8L0V8ZK_8nMyAAcXzZVBgRnFcmP
"""

# import all important libraries for analysis and modelling

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score ,f1_score
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score

from imblearn.over_sampling import SMOTE

# Load the dataset
df = pd.read_csv("/content/Bank_Churn_Modelling.csv")
df.head()

# Check the dimension of the dataset
df.shape

"""Total number of records/rows present in the dataset is: **10000**

Total number of attributes/columns present in the dataset is: **14**
"""

# Lets get the attributes in the dataset and understand each column
df.columns

"""**Let's understand Column definitions**

**RowNumber**: A unique identifier for eac\h record.

**CustomerId**: ID to track and differentiate individual customers within the dataset.

**Surname**: Family name of each customer.

**CreditScore**: Numerical value that assesses the creditworthiness of customer based on their credit history.

**Geography**: Customer's geographic distribution.

**Gender**: Customers gender data as either male or female.

**Age**: Customer's age in years and can be used to analyze age-related patterns and behaviors.

**Tenure**: The number of years or months the customer has been associated with the bank.

**Balance**: The amount of money in the customer's bank account.

**NumOfProducts**: This column include various products such as savings accounts, loans, credit cards, etc.

**HasCrCard**: This column has binary variable with a value of 1 if the customer possesses a credit card and 0 otherwise.

**IsActiveMember**: It repressnts binary variable indicating whether the customer is an active member (1) or not (0) with the bank.

**EstimatedSalary**: Customer's salary data, which can be relevant for analyzing churn behavior.

**Exited**: Whether a customer has churned (1) or not (0) from the bank. It is the variable we aim to predict using the other features.


"""

# Now lets understand the data types
df.info()

# Get missing values in the data
df.isnull().sum()

"""**Analysis**

No column has missing values, so "Data Imputation" is not required.

If our dataset has missing values, we need to handle them carefully as it can negatively impact the performance of our machine learning model.

we can use the following steps,

1) Understand the context of the data missing.

2) Missing data is completely random and has no relation with other data.

3) Missing data is related to observed data.

4) Related to missing value itself.

After analysing this, we can do following actions for Numerical and Categorical columns.

Numerical columns

- Drop rows or columns with too many missing values if they are not critical.
- Replace missing values with the mean, median, or mode

Categorical columns

- We can use Mode value,replace missing values with the most frequent category.
- Assign missing values a new category, e.g."Unknown"





"""

#Check duplicate rows
df.duplicated().sum()

"""**Analysis**

Since there no duplicate data, we can say no "Data Leakage" in our dataset.

If there are any duplicates in the data, we need to remove them to prevent leakage.
"""

# Analyse categorical columns
df.describe(include='object')

"""After analysing categorical data, we can say

**Surname** column has high unique values, and not relevant for predicting customer churned or not, so we can simply drop this feature.

**Geography & Gender** columns have low unique values and are relevant for predicting customer churned or not.

# Let's begin the ETL proces

a.	Data cleaning.

b.	Handling missing values.

c.	Handling inconsistency in the data.
"""

df.sample(10)

"""Analysis for data cleaning

**RowNumber** and **CustomerId** columns represent unique values and are not relevant for predicting customer churned or not.

**Surname** column has high unique values and also not relevant for prediction.

So we will drop this columns from our dataset.




"""

# Dropping insignificant columns
df.drop(columns=["RowNumber", "CustomerId", "Surname"], inplace=True)
df.head()

#Rename target column Exited

df.rename(columns={"Exited": "Churned"}, inplace=True)
df.head()

df["Churned"].replace({0:"No",1:"Yes"},inplace=True)
df.head()

"""We have now completed the **ETL** process and converted raw data into structured data.

So we can perform **Expolatory Data Analysis** and **derive insights from the data.**
"""

# Visualize target variable "Churned"

count = df["Churned"].value_counts()

plt.figure(figsize=(18, 6))
plt.subplot(1, 2, 1)
xy = sns.countplot(x = "Churned", data = df, hue="Churned", palette="Set2")
for label in xy.containers:
    xy.bar_label(label)
plt.title("Churned data",fontweight="black",size=20,pad=20)

"""**Analysis**

There is a huge class imbalance in churned data column which can lead to bias, so to overcome bias we will use over sampling techniques like **"Random Sampling"** or "**SMOTE**"

if we use random sampling technique ratio of data imbalance is 80:20 which may lead to overfitting.

on the other hand SMOTE will generate synthetic "Churn" records by interpolating between actual churn customer data points which reduce overfitting.

So we will go for SMOTE oversampling method.
"""

# Visualising by Gender

def countplot(column):
    plt.figure(figsize=(12,6))
    xy = sns.countplot(x=column, data=df, hue="Churned",palette="Set2")
    for label in xy.containers:
      xy.bar_label(label)
    plt.title(f"Customer Churned by {column}",fontweight="black",size=20,pad=20)
    plt.show()

countplot("Gender")

"""**Analysis**

1) Churned probability is more in females than males.

2) Banks can design targeted marketing campaigns and improve customer experience for females.
"""

countplot("Geography")

"""**Analysis**


1) Approx half of the customers are from france.

2) Despite huge total customer difference in France and Germany, Churn rate is same.

3) Almost equal customers in Spain and Germany but Churn rate is double in Germany.

"""

countplot("NumOfProducts")

"""**Analysis**

Very few customers have more than 2 products, we can do feature engineering by grouping the customers having products more than 2 together to reduce the class imbalance.


"""

countplot("IsActiveMember")

"""**Anaysis**

1) Churn rate in non active members is almost double than active members.

2) Customers which are not active are morely likely to deactivate their banking facilities.

3) Banks can provide regular communication and updates, and enhanced digital services so that customers remain active to the banking facilities.
"""

def continous_plot(column):
    plt.figure(figsize=(13,6))
    plt.subplot(1,2,1)
    sns.histplot(x=column,hue="Churned",data=df,kde=True,palette="Set2")
    plt.title(f"Data of {column} by Churn Status",fontweight="black",pad=20,size=15)

# Visualize churned by Age

continous_plot("Age")

"""**Analysis**

1) From the graph we clearly see that there are some outliers, distributing the data skewed towards right.It might lead to overfitting.

2) To make it normal distribution, we can use log normal transformation.

"""

# Visualize churned by Balance

continous_plot("Balance")

"""**Analysis**

1) We clearly see that more than 3000 customers have account balance equal to zero, which might lead to deactivation of accounts.

2) Using Feature engineering, we can group customers with zero account balance.

So we have now analysed all the variables in the dataset, we can now start the feature engineering process.
"""

# Feature Engineering Process

# Creating new features from "NumOfProducts"

con = [(df["NumOfProducts"]==1), (df["NumOfProducts"]==2), (df["NumOfProducts"]>2)]

val = ["One product","Two Products","More Than 2 Products"]

df["Total_Products"] = np.select(con,val)

df.drop(columns=["NumOfProducts"],inplace=True)

df.head()

countplot("Total_Products")

# Create new feature for Balance column

con = [(df["Balance"]==0), (df["Balance"]>0)]

val = ["Zero Balance","More Than zero Balance"]

df["Account_Balance"] = np.select(con,val)

df.drop(columns=["Balance"],inplace=True)

df.head()

countplot("Account_Balance")

"""# Data Preprocessing"""

# We wil find the unique values of Categorical columns

categorical_columns = ["Geography","Gender","Total_Products","Account_Balance"]

for columns in categorical_columns:
  print(f"{columns} has {df[columns].unique()} as unique values")

# As all the categories are independent we will apply One Hot Coding for categorical values

df = pd.get_dummies(columns=categorical_columns, data=df)

df["Churned"].replace({"No":0,"Yes":1},inplace=True)

df.head()

# As per EDA we will perform Log transformation on Age column

age_st = df["Age"]

df["Age"] = np.log(df["Age"])

# Lets now visualize before and after transformation

plt.figure(figsize=(13,6))
plt.subplot(1,2,1)
sns.histplot(age_st, color="purple", kde=True)
plt.title("Age Distribution Before Transformation",fontweight="black",size=18,pad=20)

plt.subplot(1,2,2)
sns.histplot(df["Age"], color="purple", kde=True)
plt.title("Age Distribution After Transformation",fontweight="black",size=18,pad=20)
plt.tight_layout()
plt.show()

"""We clearly see that **Age** column has acheived **normal distribution**, which will help machine learning model to find relevant patterns to build accurate model."""

# For Model training we need to segregate Features and Labels

X = df.drop(columns=["Churned"])
y = df["Churned"]

# We will split the data into Training and Testing data

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

print("Shape of x_train is:",x_train.shape)
print("Shape of x_test is: ",x_test.shape)
print("Shape of y_train is:",y_train.shape)
print("Shape of y_test is: ",y_test.shape)

"""**Analysis**

For model training and testing data is equally splitted, now we can build a **Predictive Model** to find key factors that influence customers to churn.

"""

# Now we handle class imbalance for Churned column by SMOTE oversampling technique

st = SMOTE(random_state=42)

x_train_sm, y_train_sm = st.fit_resample(x_train, y_train)

print("Shape of x_train_sm is:",x_train_sm.shape)
print("Shape of y_train_sm is:",y_train_sm.shape)

y_train_sm.value_counts().to_frame()

"""**Analysis**

Both the categories in Target variable are now having equal number of records. Now we can train the Model with balanced records for both churn categories and make a predictive model with low bias.

# Model creation with Random Forest
"""

rf = RandomForestClassifier(random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],         # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],        # Maximum depth of the trees
    'min_samples_split': [2, 5, 10],        # Minimum samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],          # Minimum samples required to be at a leaf node
    'bootstrap': [True, False],             # Whether to use bootstrap samples
    'criterion': ['gini', 'entropy']        # Criterion for splitting
}

"""**Effect on Model:**

**Gini**: Measures the impurity of a node. Faster computation.

**Entropy**: Uses information gain. Slightly more computationally intensive.
"""

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

grid_search.fit(x_train_sm, y_train_sm)

best_parameters = grid_search.best_params_

best_parameters

"""Lets build Random Forest model with best parameters."""

rf = RandomForestClassifier(**best_parameters)

rf.fit(x_train_sm, y_train_sm)

# Finding the model Accuracy

y_train_pred = rf.predict(x_train_sm)
y_test_pred = rf.predict(x_test)

print("Accuracy Score of Model on Training Data is =>",round(accuracy_score(y_train_sm,y_train_pred)*100,2),"%")

print("Accuracy Score of Model on Testing Data  is =>",round(accuracy_score(y_test,y_test_pred)*100,2),"%")

# Now lets evaluate model with different matrices

print("F1 Score of the Model is =>",f1_score(y_test,y_test_pred,average="micro"))

print("Recall Score of the Model is =>",recall_score(y_test,y_test_pred,average="micro"))

print("Precision Score of the Model is =>",precision_score(y_test,y_test_pred,average="micro"))

"""# Explore importance of features Random Forest Model"""

imp_features = pd.DataFrame({"Feature Name":x_train.columns,
                            "Importance":rf.feature_importances_})

ft = imp_features.sort_values(by="Importance",ascending=False)

plt.figure(figsize=(15,10))
sns.barplot(x="Importance", y="Feature Name", data=ft, hue="Importance", palette="plasma")
plt.title("Feature importance in Model Prediction", fontweight="black", size=20, pad=20)
plt.yticks(size=12)
plt.show()

"""# Factors which significantly influence the customers to churn are

1) AGE

2) Total_Products

3) IsActiveMember

4) Geography

5) Gender

6) Balance

# Final thoughts on the overall model performance

> The model achieved a high accuracy score of above 90% on the training data, indicating a good fit to the training instances.

> The model's accuracy score near to 85% on the testing data suggests its ability to generalize well to unseen instances.

> The model achieved high F1 score, recall, and precision values, all approximately 0.8. This indicates that the model has a strong ability to correctly identify positive cases while minimizing false positives and maximizing true positives.

> The model demonstrates strong performance across multiple evaluation metrics, indicating its effectiveness in making accurate predictions and capturing the desired outcomes.

# Outcomes for the bank to evaluate to reduce Churn

> Bank can try to convince the customers to have atleast 2 banking products but not less than 2.

> Bank can launch a scheme for customers with higher ages so that they not deactivate accounts.
"""